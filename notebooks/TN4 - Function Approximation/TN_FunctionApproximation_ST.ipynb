{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Tinkering Notebook - Function Approximation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Table of content\n",
    "* ### [1. Introduction <a id=\"intro\">](#sec1)\n",
    "* ### [2. Imports <a id=\"imports\">](#sec2)\n",
    "* ### [3. Value-Function Approximation](#sec3)\n",
    " * #### [3.1 Linear Approximation with Polynomials and Fourier Basis ](#sec3_1)\n",
    " * #### [3.2 Monte-Carlo with value function approximation](#sec3_2)\n",
    " * #### [3.3 Run the agent](#sec3_3)\n",
    " * #### [3.4 Discussions](#sec3_4)\n",
    "* ### [4. On-Policy Control with Approximation](#sec4)\n",
    " * #### [4.1 Linear Approximation with Tile Coding ](#sec4_1)\n",
    " * #### [4.2 SARSA for Estimating Action-Value Function with Function Approximation](#sec4_2)\n",
    " * #### [4.3 Run the agent](#sec4_3)\n",
    " * #### [4.4 Discussions](#sec4_4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction <a id=\"intro\"> <a id=\"sec1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References to examples, figures and pages are to the Reinforcement Learning book by Sutton and Barto.\n",
    "\n",
    "This notebook focuses on function approximation for reinforcement learning. We will first illustrate usage of function approximation for prediction of value function. We will use the random walk example from Chapter 9 of the book and use polynomials (Section 9.5.1) and Fourier basis (Section 9.5.2) as features.  We will then illustrate usage of function approximation for prediction of action-value function and use it for control with Mountain-Car environment as in Chapter 10 of the book.  \n",
    "\n",
    "__Corridor Environment:__ In order to run the exampmles with the corridor environment (1D Random Walk environment of Example 9.1-Example 9.2), you need gym-RLcourse. If you have not installed gym-RLcourse before,  you can install it as follows: \n",
    "\n",
    "```bash\n",
    "git clone https://github.com/ozayca/gym-RLcourse.git\n",
    "cd gym-RLcourse\n",
    "pip3 install -e .\n",
    "```\n",
    "     \n",
    "__RandomWalk_100.pickle:__ The file named *RandomWalk_100.pickle* is needed for plotting the true value function for the corridor environment. It should be placed in the same folder with this notebook.     \n",
    "\n",
    "__Tile Coding:__ __IMPORTANT: DO NOT SKIP!__ For the example with Mountain-Car, we will use tile coding as features, see Section 9.5.4 of the book and  http://incompleteideas.net/tiles/tiles3.html for details. The code for tile coding is provided here: http://incompleteideas.net/tiles/tiles3.py-remove. Since this is not our code, we will not be distributing it. __You need to copy and paste all of the python code on this page into a file named *TileCoding.py*__ and put it into the same folder with this Jupyter Notebook in order to run the control example. \n",
    "\n",
    "__Note on Code Completition Tasks__: These tasks are tagged with TODO-STD within the code. The explanation for these taks are given in the text just before the associated code. There are also related small tasks,  see for instance VA1-VA3 just before VA4. These small tasks  are designed to help you to understand crucial concepts and variables/methods.  In most cases, it is not possible to perform the code completion task without understanding these concepts/variables. Please do not try to skip these small tasks! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Imports <a id=\"imports\"> <a id=\"sec2\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym_RLcourse\n",
    "import pickle # used to read pickle data\n",
    "import TileCoding as tc\n",
    "from mpl_toolkits.mplot3d import Axes3D # used for 3D cost-to-go plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a corridor: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Corridor-v0', n_starting_states=10, max_delta=2)\n",
    "print('State space:', env.observation_space)\n",
    "print('Action space:', env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the corridor environment,  there are `n_starting_states` non-terminal states. The available actions are up to `max_delta` steps to the right and up to `max_delta` steps to the left. There are two terminating states, one in the left and one in the right. Transition into the left and right terminating states gives a reward of -1 of 1, respectively. All other steps give a reward of 0. If an agent chooses actions uniformly randomly from the `2 max_delta` possible actions (i.e. the probability of selection any action is the same), the agent performs a random walk on the corridor. With `n_starting_states =1000`, and `max_delta=100`, this setting corresponds to the 1000-state Random Walk setting of Example 9.1-9.2. \n",
    "\n",
    "Let's make a basic check on our installation for the corridor environment. When you run the code, you will see `n_starting_states` non-terminal states,  a terminal state on the left (TL) and another terminal state on the right (TR). Agent's location is indicated by `x` which is (approximately) halfway in the corridor at initialization. Other states are marked with o.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Value-Function Approximation <a id=\"sec3\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We now consider the prediction problem using the parametric approximation of the value function $\\hat{v}(s,w)$ with the goal of obtaining  $\\hat{v}(s,w)\\approx v_\\pi(s)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Linear Approximation with Polynomials and Fourier Basis  <a id=\"sec3_1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first implement a linear approximator that uses either  polynomials or Fourier basis as features. \n",
    "      \n",
    "__Task-VA1:__ Below, variable `method` determines whether we use Fourier features or polynomial features. Compare `construct_basis` with the equations in Section 9.5.1 and Section 9.5.2 to understand how the features are implemented. The code uses `lambda` expressions, which is just another way to define functions. You can read about `lambda` expressions in Section 4.7.6 from here https://docs.python.org/3/tutorial/controlflow.html. With this implementation, we obtain a dictionary of basis functions which we can then evaluate for any value of state. \n",
    "\n",
    "__Task-VA2:__ Find which methods/variables in the code give the following: i)  $x(s)$ for a given state ii) $\\hat{v}(s,w)$ for a given state. \n",
    "    \n",
    "__Task-VA3:__ Consider the approximate value function $\\hat{v}(s,w) = w^T x(s)$, where $w$ are the weights and $x(s)$ is the feature vector corresponding to state $s$. Show that the gradient of $\\hat{v}(s,w)$ with respect to $w$ is given by $x(s)$, i.e. $\\nabla \\hat{v}(s,w) = x(s)$.\n",
    "    \n",
    "__Task-VA4:__ Complete the `update` function so that it implements Eqn. 9.7.  \n",
    "   \n",
    "__Task-VA5:__ In the implementation below, we have a variable called `StateRange` which normalizes the values of the states. Why do you think we may found it useful to include such a variable?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearApproximator:\n",
    "\n",
    "    def __init__(self, order, method=\"Fourier\", alpha=1e-4, stateRange=1):\n",
    "        self.alpha = alpha\n",
    "        self.method = method\n",
    "        self.order = order\n",
    "        self.weights = np.zeros(order + 1)\n",
    "        self.stateRange = stateRange\n",
    "        self.construct_basis()\n",
    " \n",
    "    def construct_basis(self):\n",
    "        self.basis=[]\n",
    "        if self.method == \"Polynomial\":\n",
    "            for i in range(0, self.order + 1):\n",
    "                self.basis += [lambda s, i=i: np.power(s, i)]\n",
    "        if self.method == \"Fourier\":\n",
    "            for i in range(0, self.order + 1):\n",
    "                self.basis += [lambda s, i=i: np.cos(np.pi * s * i) ]\n",
    "\n",
    "    # return the values of features for a given state \n",
    "    def features(self, state): \n",
    "        state = state / self.stateRange  # normalize to [0, 1]\n",
    "        return np.array([f(state) for f in self.basis])\n",
    "\n",
    "    # return the value of the approximation for the given state\n",
    "    def value(self, state):  \n",
    "        return np.dot(self.features(state), self.weights)\n",
    "\n",
    "    # update the weights\n",
    "    def update(self, target, state):  # update the weights using the eqn 9.7 where U_t is the target\n",
    "        # TODO-STD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Monte-Carlo with value function approximation <a id=\"sec3_2\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We now implement the Gradient Monte Carlo Algorithm on Page 202 using a random policy (i.e. all possible actions have the same probability).  \n",
    "\n",
    "**Task-MC**: Implement the missing parts of the `learn` method which are marked with TODO-STD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCAgent:\n",
    "    # Accepts 'plain' or any method that LinearApproximator accepts. If the method is 'plain',\n",
    "    # no function approximation is performed. The 'plain' option is included for comparison purposes.\n",
    "    \n",
    "    def __init__(self, n_states, n_actions, gamma, method, order, alpha, stateRange):\n",
    "        self.n_actions = n_actions\n",
    "        self.V = np.zeros(n_states)\n",
    "        self.S = np.zeros(n_states)\n",
    "        self.N = np.zeros(n_states)\n",
    "        self.n_states = n_states\n",
    "        self.gamma = gamma\n",
    "        self.method = method \n",
    "        self.func_approx = LinearApproximator(order, method, alpha, stateRange)\n",
    "\n",
    "    def act(self, state):\n",
    "        return np.random.choice(self.n_actions)\n",
    "\n",
    "    def learn(self, states, actions, rewards):\n",
    "        T = len(states) - 1\n",
    "        if self.method == \"plain\":   #  function approximation is NOT performed\n",
    "            G = 0\n",
    "            for t in reversed(range(T)):\n",
    "                G = self.gamma * G + rewards[t + 1]  # G_t\n",
    "                self.N[states[t]] += 1\n",
    "                self.V[states[t]] += 1 / self.N[states[t]] * (G - self.V[states[t]])\n",
    "        else:  #  function approximation is performed\n",
    "            # TODO-STD Using the arrays of states, actions, rewards, write the code for the following: \n",
    "            #  1) implement the last two lines of MC algorithm on page 202.  \n",
    "            #  2) update self.V[state] so that it gives the approximate value for all states\n",
    "            # HINTS: \n",
    "            # -- Our solution uses methods of LinearApproximator, which are called using self.func_approx.update(), self.func_approx.value()\n",
    "            # using appropriate inputs. \n",
    "            # -- Note about indexing:  \n",
    "            # The train function will follow the indexing in the book. \n",
    "            # Hence, S_t, A_t, R_t+1 are at the indices t, t, t+1 in their respective arrays. \n",
    "            \n",
    "            \n",
    "            \n",
    "                \n",
    "                \n",
    "            \n",
    "                \n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Run the agent <a id=\"sec3_3\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We now create the environment. The code run time needed for obtaining illustrative results depends on the environment parameters.  We use the below parameters since they allow us to illustrate all the fundamental aspects we would like to explore in a reasonable time. After experimenting with these values, you can run the code with  `n_starting_states =1000`, and `max_delta=100` to obtain exactly the same environment setting in Example 9.1-9.2 and Figure 9.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Corridor-v0', n_starting_states=100, max_delta=10)\n",
    "stateRange = env.observation_space.n - 2\n",
    "agent = MCAgent(env.observation_space.n, env.action_space.n, gamma=1, method=\"Fourier\", order=5, alpha=5e-5,\n",
    "                stateRange=stateRange)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now run the agent on the environment. __Note that this may take some time, wait until you see 'Finished'.__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nEpisode = 5000\n",
    "max_nStep = int(2.0e6)\n",
    "print('Run starts')\n",
    "for iEpisode in range(1,nEpisode+1):\n",
    "    if iEpisode % 500 == 0:\n",
    "        print('Episode', iEpisode)\n",
    "    state = env.reset()\n",
    "    stateA = [state]\n",
    "    action = agent.act(state)\n",
    "    actionA = [action]\n",
    "    rewardA = [0]\n",
    "    done = False\n",
    "    t_done = -1\n",
    "    while not done:\n",
    "        state, reward, done, info = env.step(action)\n",
    "        stateA.append(state)\n",
    "        rewardA.append(reward)\n",
    "        action = agent.act(state)\n",
    "        actionA.append(action)\n",
    "        if t_done > max_nStep:\n",
    "            done = True\n",
    "        t_done += 1\n",
    "    agent.learn(stateA, actionA, rewardA)\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compare the learned value function with the true value function. \n",
    "\n",
    "__Note:__ The correct value function is provided for the environment parameters used in this notebook. Hence, it is not the same with the one provided Figure 9.1 which is for __1000__ non-terminal states. \n",
    "\n",
    "__Task-TVF:__ Suggest a method to find the true value function using our knowledge of the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('RandomWalk_100.pickle', 'rb') as file: # load the pre-calculated true values\n",
    "    v_true = pickle.load(file)\n",
    "\n",
    "plt.plot(v_true[1:-1],label='True')\n",
    "plt.plot(agent.V[1:-1],label='Prediction')\n",
    "plt.grid(True)\n",
    "plt.xlabel('States')\n",
    "plt.ylabel('Value Function')\n",
    "plt.ylim(-1.1, 1.1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Discussions <a id=\"sec3_4\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We will now compare our results with the ones in Figure 9.5. Nevertheless, note that we  should not expect to obtain exactly the same results with the book. In particular, note the following: i) We do not have any averaging over multiple runs, hence there is considerable randomness in our results. (Hence, after your first experiments, changing the code to average out multiple runs can be a good idea )  ii) Our corridor is a scaled-down version of the one in the book. \n",
    "\n",
    "\n",
    "__Task-PD1__:   An important aspect is the definition of $\\overline{VE}$ in the book, see Eqn 9.1.  A typical realization of the state distribution under a random agent can be seen in Figure 9.1. According to the Figure 9.1, value function estimates of which states affect $\\overline{VE}$ more? State whether you think this is a good way of defining the error.  \n",
    "\n",
    "__Task-PD2__:   Relate $\\overline{VE}$ with  $E_\\pi[(v_\\pi(S)−\\hat{v}(S,w))^2]$ defined on the lecture slides. \n",
    "\n",
    "__Task-PD3__:   Repeat the experiment with Fourier basis and Polynomial basis with the order of 5, 10 and 20  and multiple times.  In terms of the general trends, are the results consistent with Figure 9.5? In particular, check the following: \n",
    "<ul>\n",
    "<li> The book states that polynomials do not provide satisfactory results in RL tasks. The results in Figure 9.5 support this claim: Fourier basis shows better performance than polynomials. Are our results consistent with these claims? </li>\n",
    "<li> Do  higher orders provide significantly better predictions?</li>\n",
    "</ul>\n",
    "\n",
    "__Task-PD4__: Run the usual MC agent without function approximation using the option `method= 'plain'`. How does the value function prediction found in this case compare with the predictions using approximation? Which method(s) are *better*? Do your conclusions depend on the length of the corridor and the maximum allowed movement? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. On-Policy Control with Approximation <a id=\"sec4\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We focus on the control problem using the parametric approximation of the action-value function $\\hat{q}(s,a,w)$ with the aim of obtaining $\\hat{q}(s,a,w) \\approx q_*(s,a)$. \n",
    "\n",
    "Consider Eqn 9.7. We now would like to now approximate the action-value function $q(s,a)$ instead of the value function $v(s)$. Hence, the update will be the following: \n",
    "\n",
    "$$ w_{t+1} = w_t + \\alpha [U_t - \\hat{q}(S_t, A_t, w_t)] \\nabla \\hat{q}(S_t, A_t, w_t)$$\n",
    "    \n",
    "We will refer to this eqn as Eqn 9.7Q. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Linear Approximation with Tile Coding  <a id=\"sec4_1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We now implement a linear approximator that uses tile coding. Since tile coding operates in a slightly different manner than Fourier basis/polynomial, we provide a new linear approximator class for ease of exposition. Note that here indices of active tiles are used to represent value function of different states. See Section 9.5.4 of the book and  http://incompleteideas.net/tiles/tiles3.html for details. \n",
    "\n",
    "\n",
    "    \n",
    "__Task-TC1:__ Examine the usage of tile coding and make sure that you understand how we use the indices of active tiles as features. In particular, check the method `indicesActiveTiles`.\n",
    "\n",
    "__Task-TC2:__ Consider the method `value`. Verify that this method gives the approximation $\\hat{q}(S_t, A_t, w_t) $ for a given state, action pair. \n",
    "\n",
    "__Task-TC3:__ Tile coding is just another set of features. Explain why the `value` method looks different than the one for Fourier basis and polynomials.  \n",
    "    \n",
    "__Task-TC4:__ Complete the `update` function using Eqn 9.7Q (This eqn is defined in this notebook). Note that this `update` function may look different than the `update' function we had for Fourier basis and polynomials. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearApproximatorTile:\n",
    "    def __init__(self, nTiling=8, size=4096, alpha=0.3, stateRange=1):\n",
    "        self.size = size\n",
    "        self.nTiling = nTiling\n",
    "        self.iht = tc.IHT(size)\n",
    "        self.weights = np.zeros(size) \n",
    "        self.alpha = alpha\n",
    "        self.stateRange = stateRange\n",
    "\n",
    "        # scaling for the states, see ``Fleshing out the example\" section on http://incompleteideas.net/tiles/tiles3.html\n",
    "        # For the mountain-car example also see footnote 1 on page 246 of SuttonBarto_2018\n",
    "        scaleFactor = self.nTiling\n",
    "        self.scale = [scaleFactor / s for s in self.stateRange]\n",
    "\n",
    "    # get indices of active tiles (i.e features)\n",
    "    def indicesActiveTiles(self, state, action):\n",
    "        scaledState = [s * scale for s, scale in zip(state, self.scale)]\n",
    "        return tc.tiles(self.iht, self.nTiling, scaledState, [action])\n",
    "\n",
    "    # calculate q_app(state, action)\n",
    "    def value(self, state, action):\n",
    "        ind = self.indicesActiveTiles(state, action)\n",
    "        return np.sum(self.weights[ind])\n",
    "\n",
    "    # update the weights \n",
    "    def update(self, target, state, action):  # update the weights using  Eqn 9.7Q\n",
    "        # TODO-STD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 SARSA for Estimating Action-Value Function with Function Approximation <a id=\"sec4_2\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The following class implements the algorithm *Episodic semi-gradient Sarsa for estimating $q_*$*  on page 244 of the book. \n",
    "\n",
    " Task-Sarsa: Complete the `learn` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSA:\n",
    "    def __init__(self, gamma, epsilon, alpha, stateRange):\n",
    "        self.actions = range(3)  # 0,1,2:  reverse, stay, go forward\n",
    "        self.state = [0, 0]  # position, velocity\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.func_approx = LinearApproximatorTile(nTiling=8, size=4096, alpha=alpha, stateRange=stateRange)\n",
    "        self.done = False\n",
    "\n",
    "    def act(self, state): \n",
    "        if np.random.random_sample() <= self.epsilon:  # random action wp epsilon  \n",
    "            action = np.random.choice(self.actions)\n",
    "        else:  # greedy action wp 1-epsilon\n",
    "            action_values = []\n",
    "            for action in self.actions:\n",
    "                action_values.append(self.func_approx.value(state, action))\n",
    "            max_value = np.max(action_values)\n",
    "            indices_bestValue = [i for i, j in enumerate(action_values) if j == max_value]\n",
    "            action = np.random.choice(indices_bestValue)\n",
    "        return action\n",
    "\n",
    "    # Sutton, Barton pg.244\n",
    "    def learn(self, state, action, reward, state_next, action_next, doneEpisode): \n",
    "        # TODO-STD Implement the weight updates according to Sarsa pseudo-code on page 244\n",
    "        # Note that you need to think of the cases the episode is finished or not seperately\n",
    "        # HINT: Our implementation uses self.func_approx.value() and self.func_approx.update() with appropriate inputs\n",
    "        if doneEpisode == True:   #  state_next is a terminal state\n",
    "           # ....\n",
    "        else: \n",
    "             # ....\n",
    "   \n",
    "\n",
    "\n",
    "    # Calculate cost-to-go \n",
    "    # With a reward of -1 each step, this corresponds to expected minimum number of steps \n",
    "    # required for reaching the goal. See, for instance, Fig 10.1, pg 245.\n",
    "    def costToGo(self, state):\n",
    "        q = []\n",
    "        for action in self.actions:\n",
    "            q.append(self.func_approx.value(state, action))\n",
    "        return -np.max(q)\n",
    "\n",
    "    # !Caution: This plot function assumes a 2-D state space\n",
    "    def plot_costToGo(self, xA, yA, x_label, y_label):\n",
    "        xsA, ysA, zsA = [], [], []  # initialize with empty lists\n",
    "        for x in xA:\n",
    "            for y in yA:\n",
    "                state = [x, y]\n",
    "                xsA.append(x)\n",
    "                ysA.append(y)\n",
    "                zsA.append(self.costToGo(state))\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax = Axes3D(fig)\n",
    "        ax.scatter(xsA, ysA, zsA)\n",
    "\n",
    "        ax.set_xlabel(x_label)\n",
    "        ax.set_ylabel(y_label)\n",
    "        ax.set_zlabel(\"Cost-to-go\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now introduce the training function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, agent, nEpisode):\n",
    "    total_reward_episodes = np.zeros(nEpisode)\n",
    "    max_nStep = 1000\n",
    "    print('Run starts')\n",
    "    for iEpisode in range(nEpisode):\n",
    "        if iEpisode % 50 == 0:\n",
    "            print('Episode', iEpisode)\n",
    "        done = False\n",
    "        doneEpisode = False\n",
    "        t = 0\n",
    "        state = env.reset()\n",
    "        action = agent.act(state)\n",
    "        actionA = [action]\n",
    "        stateA = [state]\n",
    "        rewardA = [0]\n",
    "        while not done:\n",
    "            state_next, reward, doneEpisode, info = env.step(action) \n",
    "            stateA.append(state_next)\n",
    "            rewardA.append(reward)\n",
    "            action_next = agent.act(state_next)\n",
    "            actionA.append(action_next)\n",
    "            agent.learn(state, action, reward, state_next, action_next, doneEpisode)\n",
    "            action = action_next\n",
    "            state  = state_next\n",
    "            t += 1\n",
    "            if t > max_nStep:\n",
    "                done = True\n",
    "            if doneEpisode == True:\n",
    "                done = True\n",
    "        total_reward_episodes[iEpisode] = np.sum(rewardA)\n",
    "    return (total_reward_episodes, stateA)\n",
    "    print('Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Run the agent <a id=\"sec4_3\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run this agent in Mountain Car task where the goal is to pass to the right of the position point 0.5. Please see Example 10.1 for details of the environment.  \n",
    "\n",
    "We create the environment,  prepare some parameters and create the agent. The learning parameter $\\alpha$ is chosen using Figure 10.2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'MountainCar-v0'  in default terminates the episodes with 200 steps. Calling with \".env\" gets rid of this limitation.\n",
    "#   A successful RL approach will be able to learn with 200 steps.  We allow changing the number of steps only for experimentation \n",
    "#   and to be able to make direct comparisons with Example 10.1 where maximum number of steps is at least 1000 \n",
    "#   (You can look at Figure 10.2 to see this). \n",
    "env = gym.make('MountainCar-v0').env\n",
    "print('State space:', env.observation_space)\n",
    "print('Action space:', env.action_space)\n",
    "\n",
    "# state limits for Mountain-Car Environment\n",
    "min_position = -1.2\n",
    "max_position = 0.6  # Gym 'MountainCar-v0' uses 0.6 instead of 0.5 of the book\n",
    "max_velocity = 0.07\n",
    "min_velocity = -0.07\n",
    "stateRange = [max_position - min_position, max_velocity - min_velocity]\n",
    "\n",
    "# Sampling of the state-space for cost-to-go plots\n",
    "n_sample_plot = 60\n",
    "posA_plot = np.linspace(min_position, max_position, n_sample_plot)\n",
    "velA_plot = np.linspace(min_velocity, max_velocity, n_sample_plot)\n",
    "\n",
    "agent = SARSA(gamma=1, epsilon=0, alpha=0.5/8, stateRange=stateRange)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train. __Note that training may take some time, wait until you see 'Finished'.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reward_episodes, stateA = train(env, agent, nEpisode = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define some plot functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the position and velocity for MountainCar\n",
    "def plotTrajectory(stateA):  \n",
    "    posA = [state[0] for state in stateA]\n",
    "    velA = [state[1] for state in stateA]\n",
    "\n",
    "    plt.plot(posA, 'o')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Position')\n",
    "    plt.ylim(-1.3, 0.6)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(velA, 'o')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Velocity')\n",
    "    plt.ylim(-0.075, 0.075)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Plot minus of total reward which corresponds to steps per episode for Mountain Car Environment\n",
    "def plotStepsPerEpisode(total_reward_episodes): \n",
    "    plt.plot(-total_reward_episodes)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Steps per episode')\n",
    "    plt.ylim(0, 1000)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visually inspect the last trajectory. Has the car reached the goal? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotTrajectory(stateA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the cost-to-go function learned in this run. Has the agent correctly identified the general behaviour of the expected number of steps before reaching the goal in different parts of state-space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.plot_costToGo(posA_plot, velA_plot, 'Position', 'Velocity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now plot the reward  during training. Note that the reward corresponds to total number of steps per episode. In terms of general trends, does the number steps per episode decrease during the training? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotStepsPerEpisode(total_reward_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Discussions <a id=\"sec4_4\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We now compare our results with the figures in the book. Similar to the before, note the following: in this notebook we do not perform averaging over runs hence we typically have more randomness in our results compared to the book. We should keep this point in mind while interpreting our results. You are encouraged  to perform averaging over the runs after your initial experiments.\n",
    "\n",
    "__Task-CD1:__  Similar to Figure 10.1, plot the cost-to-go function at different steps in the first episode and at the end of different episodes.  Verify that you can see the evolution of this function from a flat function of all zeros to functions similar to the ones Figure 10.1. You can create these plots using `agent.plot_costToGo(posA_plot, velA_plot, 'Position', 'Velocity')` at any point after creating the agent.\n",
    "\n",
    "__Task-CD2:__  Consider Figure 10.2 which shows how the episode length varies with different learning rates as the agent trains. Try different $\\alpha$ values for the agent. Do you observe the same type of behaviour? Explain the reason behind the general trend as $\\alpha$ increases/decreases. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
