{"cells": [{"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["# Tinkering notebook 2: Markov Decision Processes and Dynamic Programming\n", "In this notebook we will see how some of the content of Lecture 2 - Lecture 3 works in practice. \n", "\n", "We will start by a repetition of Markov Decision Processes (MDPs) and value functions. \n", "After this we will study two GridWorld examples (Example 4.1 in the textbook and FrozenLake). Here we will use dynamic programming to learn both value functions and optimal policies."]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["# Table of content\n", "* ### [1. Imports](#sec1)\n", "* ### [2. Markov Decision Processes and Value Functions](#sec2)\n", " * #### [2.1 The Bellman equations](#sec2_1)\n", " * #### [2.2 Example: Study or Facebook?](#sec2_2)\n", " * #### [2.3 *Analytical solution to the Bellman equation](#sec2_3)\n", "* ### [3. Helper functions](#sec3)\n", "* ### [4. The environments](#sec4)\n", " * #### [4.1 Example 4.1: GridWorld-v0](#sec4_1)\n", " * #### [4.2 The Frozen Lake](#sec4_2)\n", "* ### [5. MDPs and the Bellman equations](#sec5)\n", " * #### [5.1 Test your code on Example 4.1 (GridWorld)](#sec5_1)\n", "* ### [6. Policy Evaluation](#sec6)\n", " * #### [6.1 In place updates](#sec6_1)\n", "* ### [7. Policy Iteration](#sec7)\n", "* ### [8. Value iteration](#sec8)\n"]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["# 1. Imports <a id=\"sec1\">"]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["The packages needed in this notebook are:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Packages needed for this notebook\n", "import gym\n", "import gym_gridworld\n", "import numpy as np\n", "import time\n", "import random\n", "import matplotlib.pyplot as plt\n", "from IPython.display import clear_output # Used to clear the ouput of a Jupyter cell."]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["# 2. Markov Decision Processes and Value Functions <a id=\"sec2\">"]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["An MDP consist of a state space $\\mathcal{S}$, an action space $\\mathcal{A}$, a reward set $\\mathcal{R}$ and a transition function $p(s', r | s, a)$. We define the return as \n", "$$\n", "G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots\n", "$$\n", "where $\\gamma$ is the discount rate.\n", "\n", "**State-value function:**\n", "$$\n", "v_{\\pi}(s) = \\mathbb{E}_{\\pi}[ G_t | S_t = s]\n", "$$\n", "The expected return starting from state $s \\in \\mathcal{S}$ and following policy $\\pi$.\n", "\n", "**Action-value function ($Q$-value)**:\n", "$$\n", "q_{\\pi}(s) = \\mathbb{E}_{\\pi}[ G_t | S_t = s, A_t = a]\n", "$$\n", "The expected return starting from state $s \\in \\mathcal{S}$, then taking action $a \\in \\mathcal{A}$ and then follow policy $\\pi$."]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["## 2.1 The Bellman equations <a id=\"sec2_1\">"]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["We have seen that $v_{\\pi}(s)$ is the solution to the Bellman equation\n", "$$\n", "\\begin{aligned}\n", "v_{\\pi}(s) &= \\mathbb{E}_{\\pi}[ R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) | S_t = s] \\\\\n", "&= \\sum_{a} \\pi(a|s) \\sum_{r} \\sum_{s'} p(s', r | s, a) [ r + \\gamma v_\\pi(s') ] = \\sum_{a} \\pi(a|s) q_{\\pi}(s,a)\n", "\\end{aligned}\n", "$$\n", "where\n", "$$\n", "q_{\\pi}(s,a) = \\mathbb{E}_{\\pi}[ G_t | S_t = s, A_t = a] = \\sum_{r}\\sum_{s'} p(s',r | s,a) [ r + \\gamma v_{\\pi}(s')]\n", "$$"]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["## 2.2 Example: Study or Facebook? <a id=\"sec2_2\">"]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["In Lecture 2 we looked at the example MDP bellow.\n", "<img src=\"example.png\">\n", "\n", "It has four states, $\\mathcal{S} = \\{ K_0, K_1, K_2, Pass \\}$. The state $Pass$ is a terminal state, so the episode will end if this state is reached (alternatively, if we reach $Pass$ we will stay there forever and receive 0 future return).\n", "\n", "In each non-terminal state we can choose between `Study` or `Facebook`, $\\mathcal{A} = \\{ Study, Facebook\\}$. The red nodes corresponds to actions, and the labels on the edges gives immediate rewards (green) and transition probabilities (black)."]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["**Task:** Assume that we use the policy $\\pi(a|s) = 0.5$ for all states and actions, and the discount factor $\\gamma = 0.9$.  \n", "\n", "In Lecture 2 we saw that the state-value function (rounded to two decimals) is\n", "$$\n", "v_{\\pi}(K_0) = 3.00, \\quad v_{\\pi}(K_1) = 4.78, \\quad v_{\\pi}(K_2) = 7.84.\n", "$$\n", "1. Verify that this satisfies the Bellman equation for all states! (At least approximately, since we have rounded everything to two decimals).\n", "\n", "You can use the code-block bellow to carry out your computations."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["gamma = 0.9\n", "vK0 = \n", "vK1 = \n", "vK2 =\n", "print(\"v(K0) =\", vK0, \"v(K1) =\", vK1, \"v(K2)=\", vK2)"]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["## 2.3 *Analytical solution to the Bellman equation <a id=\"sec2_3\">"]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["Note that the Bellman equation can be written as\n", "$$\n", "v_{\\pi}(s) = \\mathbb{E}_{\\pi}[ R_{t+1} | S_{t} = s] + \\gamma\\sum_{a, s'} \\pi(a|s)p(s' | s, a) v_{\\pi}(s').\n", "$$\n", "So the value of $s$ is the average immediate reward plus the discounted average value of the next state. "]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["**Example: From the state $K_1$**:\n", "    \n", "* In $s = K_1$ the expected immediate reward is $-0.5$, since we choose `Facebook` with probability 0.5 (reward 0) and `Study` with probability 0.5 (reward -1). \n", "* There is a 0.5 probability that the action is `Facebook`, and then there is a 50/50 chance of going either to $K_0$ or $K_1$. Hence the total probability of going to $K_0$ and $K_1$ are both 0.25 ($0.5 \\times 0.5$). There is also a 0.5 probability for the action `Study` which will move us to $K_2$. Finally there is 0 probability of reaching $Pass$. \n", "\n", "Summarizing this we get\n", "$$\n", "v_{\\pi}(K_1) = -0.5 + \\gamma [ 0.25 v_{\\pi}(K_0) + 0.25 v_{\\pi}(K_1) + 0.5 v_{\\pi}(K_2) + 0 v_{\\pi}(Pass)]\n", "$$\n", "If we define a vector with all state-values\n", "$$\n", "V_{\\pi} = \\begin{bmatrix} v_{\\pi}(K_0) \\\\ v_{\\pi}(K_1) \\\\ v_{\\pi}(K_2) \\\\ v_{\\pi}(Pass) \\end{bmatrix}\n", "$$\n", "we can write this as\n", "$$\n", "v_{\\pi}(K_1) = \\underbrace{-0.5}_{r_1} + \\gamma \\underbrace{\\begin{bmatrix} 0.25 & 0.25 & 0.5 & 0 \\end{bmatrix}}_{p_1} V_\\pi\n", "$$\n", "Note that the elements of $p_1$ are the probability of going from $K_1$ to each of the other states when we follow the policy."]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["**Putting it all together:**\n", "\n", "If we do the same for all states, we get an equation on the form \n", "$$\n", "V_{\\pi} = R + \\gamma P V_{\\pi}\n", "$$\n", "where $R$ is the vector of expected immediate rewards for each state, and $P \\in \\mathbb{R}^{4 \\times 4}$ where element $P_{i,j}$ is the probability of moving from the $i$th state to the $j$th state when we follow the policy.\n", "\n", "Assuming that $\\gamma < 1$ there is always a unique solution given by \n", "$$\n", "V_{\\pi} = (I - \\gamma P)^{-1} R.\n", "$$\n", "\n", "**Task**: Fill in the correct values of $R$ and $P$ in the code below, and see if you find the same solution as in the slides of Lecture 2. You can also play around with the discount rate, and/or try to compute $R$, $P$ and then $V_{\\pi}$ for another policy.\n", "\n", "**Note**: The state $Pass$ is a bit special, since it is a terminating state. Hence, when we reach \"Pass the exam\" we will not receive anymore rewards, and we will stay in this state forever (the probability of going to any other state is 0). Hence\n", "$$\n", "v_{\\pi}(Pass) = 0 + \\gamma \\begin{bmatrix} 0 & 0 & 0 & 1 \\end{bmatrix} V_{\\pi}\n", "$$"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["discount = 0.9 # gamma\n", "R = np.zeros((4,1))\n", "P = np.zeros((4,4))\n", "\n", "# Enter the expected immediate reward for each state\n", "# Those computed in the text above are already filled in.\n", "R[0] = ? # For K_0\n", "R[1] = -.5 # For K_1\n", "R[2] = ? # For K_2\n", "R[3] = 0 # For \"Pass exam\"\n", "\n", "# Enter the probabilities going from state i to state j\n", "P[0] = [?, ?, ?, ?] # for i=0 (K_0)\n", "P[1] = [0.25, 0.25, 0.5, 0] # for i=1 (K_1)\n", "P[2] = [?, ?, ?, ?] # for i=2 (K_2)\n", "P[3] = [0, 0, 0, 1] # for i=3 (Pass the exam) \n", "\n", "# Solve the Bellman equation \n", "V = np.linalg.inv(np.eye(4) - discount*P)@R # V = (I - discount*P)^-1 * R\n", "print(V)"]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["# 3. Helper functions <a id=\"sec3\">"]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["We will now start to look at two GridWorld examples, but first we define some functions that will be useful in the rest of the notebook.\n", "\n", "The class `RandomAgent` implements an agent with a policy $\\pi(a|s)$. The method `act` will take the state $s$ as input, and then sample an action according to the probabilities $\\pi(s|a)$. \n", "\n", "To implement this we use a table `probs` of size $|\\mathcal{S}| \\times |\\mathcal{A}|$. So `probs[s][a]` $= \\pi(a|s)$. Here we implement an agent that (initially) chooses the action with a uniform probability, so all actions are equally likely in each state."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class RandomAgent():\n", "    \n", "    def __init__(self, nA=4, nS=16):\n", "        self.nA = nA # Number of actions\n", "        self.nS = nS # Number of states\n", "        \n", "        # Uniform probabilites in each state.\n", "        # That is, in each of the nS states\n", "        # each of the nA actions has probability\n", "        # 1/nA.\n", "        self.probs = np.ones((nS,nA))/nA \n", "\n", "    def act(self, state):\n", "        action = np.random.choice(self.nA, p=self.probs[state]) \n", "        return action # a random policy"]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["We also implement a function that will let the agent run on the environment:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def run_agent(env, agent, tsleep = 0.05):\n", "    state = env.reset()\n", "    time_step = 0\n", "    total_reward = 0\n", "    reward = 0\n", "    done = False\n", "    while not done:\n", "        action = agent.act(state);\n", "        state, reward, done, info = env.step(action)\n", "        total_reward += reward\n", "        time_step += 1\n", "        clear_output(wait=True)\n", "        env.render()\n", "        print(\"Time step:\", time_step)\n", "        print(\"State:\", state)\n", "        print(\"Action:\", action)\n", "        print(\"Total reward:\", total_reward)\n", "        time.sleep(tsleep)"]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["# 4. The environments <a id=\"sec4\">"]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["In this notebook we will try the methods on two environments. Both environments are $4\\times 4$ gridworlds, see figure below.\n", "\n", "<img src=\"grid.png\" width=200>\n", "\n", "The agent can be in one of the 16 grids, so the state space is\n", "$$\n", "\\mathcal{S} = \\{ 0, 1, 2, \\ldots, 15\\}.\n", "$$\n", "In each state the agent can take one out of four actions:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["LEFT = 0\n", "DOWN = 1\n", "RIGHT = 2\n", "UP = 3"]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["So the action space is \n", "$$\n", "\\mathcal{A} = \\{ 0, 1, 2, 3\\}\n", "$$\n", "Lets now look at the two different environments."]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["## 4.1 Example 4.1: GridWorld-v0 <a id=\"sec4_1\">"]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["The first environment is called `GridWorld-v0`, and is described in Example 4.1 of the textbook. To create the environment and study the state and action spaces, execute:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["env = gym.make('GridWorld-v0')\n", "state = env.reset()\n", "print(\"State space:\", env.observation_space)\n", "print(\"Action space:\", env.action_space)"]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["We can see that the state space has 16 states, and there 4 actions (as mentioned above). Let us next visualize the environment:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["env.render()"]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["`S` is the starting state.\n", "\n", "`F` is a state where the agent can walk.\n", "\n", "`G` are the two goal states. (In Example 4.1 these two states are considered to be one state).\n", "\n", "**Reward:** The agent receives the reward -1 for each action taken, and the episode ends when a goal state is reached. Hence, the agent should reach a goal state with as few actions as possible in order to maximize the total reward.\n", "\n", "**Dynamics:** This is a deterministic environment. So if the agent chooses the action `LEFT = 0` then it will move one step to the left if possible. If it hits a wall it will just stay in the same place.\n", "\n", "We can try to move the agent one step to the left with the following code:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["new_state, reward, done, _ = env.step(LEFT) # Take action LEFT = 0\n", "env.render()\n", "print(\"New state:\", new_state)\n", "print(\"Reward:\", reward)"]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["**More about dynamics:** \n", "The GridWorld environment has the variable `env.P` that encodes the dynamics $p(s', r | s, a)$ of the environment. \n", "\n", "`env.P[s][a]` will give back a list where each element is on the form `(probability, next_state, reward, terminating)`. So, if you take action `a` in state `s`, then with probability `probability` you will move to `next_state` and get the reward `reward`. `terminating` tells us if `next_state` will terminate the episode or not. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["s = 9\n", "a = LEFT\n", "print(env.P[s][a])\n", "for p, next_s, reward, _ in env.P[s][a]: # Go through all possible transitions\n", "    print(\"With probability\", p, \"you will move to state\", next_s, \n", "          \"and get the reward\", reward)"]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["Since this is a deterministic environment, the list only contains one next state that is reached with probability 1.0.\n", "\n", "Finally we can try to run the agent that chooses between all actions with equal probability in all states. Try to run it a few time, and note that you will get different total rewards every time since the agent uses a random policy."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["agent = RandomAgent(env.action_space.n, env.observation_space.n)\n", "run_agent(env, agent)"]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["By changing `agent.probs` we can change the policy of the agent. Try to implement an optimal policy (see Figure 4.1 in the textbook).\n", "\n", "Note that when two possible directions are shown in the optimal policy, then you can choose between them anyway you want (either pick one with probability 1.0, or maybe pick between them with equal probability)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["agent.probs = np.zeros((16, 4))\n", "\n", "# Note that in each state the total\n", "# probability must add upp to 1.0.\n", "\n", "# Row 1\n", "agent.probs[1][LEFT] = 1.0\n", "agent.probs[2][LEFT] = 1.0\n", "agent.probs[3][[LEFT, DOWN]] = 0.5 # Pick between them with equal probability\n", "\n", "# Row 2\n", "\n", "\n", "\n", "\n", "\n", "# Row 3\n", "\n", "\n", "\n", "\n", "\n", "# Row 4 \n", "\n", "\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["run_agent(env, agent)"]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["## 4.2 The Frozen Lake <a id=\"sec4_2\">"]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["The environment `FrozenLake-v0` [(link)](https://gym.openai.com/envs/FrozenLake-v0/) is similar to `GridWorld-v0`, but it is stochastic."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["env = gym.make('FrozenLake-v0')\n", "state = env.reset()\n", "print(\"State space:\", env.observation_space)\n", "print(\"Action space:\", env.action_space)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["env.render()"]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["`S` is the starting state.\n", "\n", "`F` is a frozen surface that the agent can walk on, but it is slippery, so the movement direction only partially depends on the action.\n", "\n", "`H` is a hole. If the agent steps here, it will fall in. (The episode terminates with $0$ reward.)\n", "\n", "`G` is the goal. If the agents steps here, the episode terminates with reward $+1$.\n", "\n", "**Reward**: All actions not leading to the goal state gives a reward of 0. Hence, to maximize the reward the agent much reach the goal state without falling into a hole."]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["**Dynamics:** Here we also have `env.P[s][a]` to see the dynamics of the environment."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["s = 9\n", "a = LEFT \n", "print(env.P[s][a])\n", "for p, next_s, reward, _ in env.P[s][a]:\n", "    print(\"With probability\", p, \"you will move to state\", next_s, \n", "          \"and get the reward\", reward)"]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["We see that the environment is stochastic in this case, since the action `LEFT` in state 9 may take the agent either to state 5 (up into a hole!), state 8 (left) or state 13 (down), due to the slippery surface.\n", "\n", "We can also try to run this environment using the random policy:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["agent = RandomAgent(env.action_space.n, env.observation_space.n)\n", "run_agent(env, agent)"]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["We can see that typically the agent ends up in one of the holes, and thus the total reward is typically 0 (but the expected total reward is positive, since there is a non-zero probability that we reach the goal)."]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["# 5. MDPs and the Bellman equations <a id=\"sec5\">"]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["In this section we will study the Bellman equations for the value function a bit closer. Remember that we defined the return as \n", "$$\n", "G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots\n", "$$\n", "and the state-value function as \n", "$$\n", "v_\\pi(s) = \\mathbb{E}[ G_t | S_{t} = s].\n", "$$\n", "The Bellman equation for the state-value function is then \n", "$$\n", "v_\\pi(s) = \\sum_{a} \\pi(a|s) \\sum_{r} \\sum_{s'} p(s', r | s, a) [ r + \\gamma v_\\pi(s') ] = \\sum_{a} \\pi(a|s) q_{\\pi}(s,a)\n", "$$\n", "where\n", "$$\n", "q_{\\pi}(s,a) = \\mathbb{E}_{\\pi}[ G_t | S_t = s, A_t = a] = \\sum_{r}\\sum_{s'} p(s',r | s,a) [ r + \\gamma v_{\\pi}(s')]\n", "$$\n", "is the action-value function."]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["**Implementation:**\n", "In the code we will represent the state-value function $v_\\pi(s)$ as a vector $v$ with one element for each state in $\\mathcal{S}$.\n", "\n", "We will now implement functions for computing the right-hand side of the Bellman equation. \n", "\n", "**Task:**\n", "Complete `compute_action_value` and `Bellman_RHS`. Make sure that you understand the code.\n", "\n", "We start by a function that computes the action values $q_{\\pi}(s,a)$ given the state-value function $v_\\pi(s)$."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def compute_action_value(env, discount, s, a, v):\n", "    \n", "    action_value = 0\n", "    \n", "    for p, next_s, reward, _ in env.P[s][a]:\n", "        # Loop through all possible (s', r) pairs\n", "        action_value += ?\n", "    \n", "    return action_value"]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["With the action values, we can now compute $\\sum_{a} \\pi(a|s) q_{\\pi}(s,a)$ (the expected action value) to get the right-hand side (RHS) of the Bellman equation.\n", "\n", "For this we use `agent.probs[s][a]` $=\\pi(a|s)$, see discussion in \"Helper Functions\"."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def Bellman_RHS(env, discount, agent, s, v):\n", "    \n", "    state_value = 0\n", "    \n", "    for a in range(env.action_space.n):\n", "        # Loop through all possible actions\n", "        state_value += ?\n", "    \n", "    return state_value"]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["Finally we implement a function that, given a value function, computes the right-hand side of the Bellman equation for all states."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def Bellman_RHS_all(env, discount, agent, v0):\n", "    # v0 is the given value function\n", "    # v will be the right-hand side of the Bellman equation\n", "    # If v0 is indeed the value function, then we should get v = v0.\n", "    \n", "    v = np.zeros(env.observation_space.n)\n", "    \n", "    for s in range(env.observation_space.n):\n", "        v[s] = Bellman_RHS(env, discount, agent, s, v0)\n", "    \n", "    return v"]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["## 5.1 Test your code on Example 4.1 (GridWorld) <a id=\"sec5_1\">"]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["For Example 4.1 we will consider the state-value $v_{\\pi}(s)$ for the policy when each action is taken with equal probability. The discount rate is\n", "$$\n", "\\gamma = 1\n", "$$\n", "The value function for this policy is given in Figure 4.1 in the textbook (the lower left), and it is"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["v = np.array([[0, -14, -20, -22], \n", "             [-14, -18, -20, -20],\n", "             [-20, -20, -18, -14], \n", "             [-22, -20, -14, 0]]).ravel()\n", "\n", "print(\"v as vector:\", v)\n", "print(\"v as matrix:\\n\", v.reshape(4,4))\n", "\n", "# ravel turns the matrix into an array,\n", "# and with reshape we print it as a matrix again so that it is easier to read."]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["We can now use our code to see if this value function really satisfy the Bellman equation:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["env = gym.make('GridWorld-v0')\n", "agent = RandomAgent()\n", "discount = 1\n", "v_new = Bellman_RHS_all(env, discount, agent, v)\n", "print('Right-hand side of Bellman equation:\\n', v_new.reshape(4,4))"]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["**Task:** `v` is the true value-function for the policy $\\pi$ implemented in `agent`. Hence, `v_new` (the right-hand side of the Bellman equation) should be equal to `v`. \n", "\n", "If `v_new` is not equal to `v`, go back and fix your code for `compute_action_value` and `Bellman_RHS`. Remember to re-run the code cells after you have changed the code!"]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["# 6. Policy Evaluation <a id=\"sec6\">"]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["In Lecture 3 we learned that one way of solving the Bellman equation, is to start from an initial guess and then repeatedly update the value function by applying the right-hand side of the Bellman equation. \n", "\n", "Below is one way to implement this. The iteration will stop when the maximum change in $v$ is less than `tol` (tolerance) or the number of iterations are `max_iter`.\n", "\n", "***Note:*** For this code to work properly, your implementation of `compute_action_value` and `Bellman_RHS` must be correct. So make sure that you have tested your code first!\n", "\n", "**Task:** Make sure that you understand the code!"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def policy_evaluation(env, discount, agent, v0, max_iter=1000, tol=1e-6):\n", "    \n", "    v_old = v0\n", "    \n", "    for i in range(max_iter):\n", "        v_new = Bellman_RHS_all(env, discount, agent, v_old)\n", "        \n", "        if np.max(np.abs(v_new-v_old)) < tol:\n", "            break\n", "            \n", "        v_old = v_new\n", "        \n", "    return v_new"]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["Lets try this on the `GridWorld-v0` example, with the uniformly random policy. We start with an initial guess $v_{\\pi}(s) = 0$ for all $s$."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["env = gym.make('GridWorld-v0')\n", "agent = RandomAgent()\n", "discount = 1\n", "v0 = np.zeros((env.observation_space.n))\n", "\n", "v = policy_evaluation(env, discount, agent, v0)\n", "print(v.reshape(4,4))"]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["Is this (approximately) the same as the true value function in Figure 4.1? $(k=\\infty)$\n", "If you do not find the correct value function, make sure that your code in `compute_action_value` and `Bellman_RHS` is correct!\n", "\n", "To replicate the other parts of Figure 4.1, you can set `max_iter` in order to see how the value function looks after a few iterations."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["v = policy_evaluation(env, discount, agent, v0)\n", "print(v.reshape(4,4))"]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["**Task:** Use `policy_evaluation` to compute the value function for `FrozenLake-v0` when the uniformly random policy is used. Use $\\gamma = 1$."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["env = gym.make('FrozenLake-v0')\n", "agent = RandomAgent()\n", "discount = 1\n", "# Write code for computing the state-value function\n", "", "", "print(v.reshape(4,4))"]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["## 6.1 In place updates <a id=\"sec6_1\">"]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["As mentioned in Lecture 3 and in the textbook, the policy evaluation is often implemented using in place updates.\n", "\n", "This can both simplify implementation, since we do not keep two separate arrays, and it can also speed up convergence.\n", "\n", "**Task:** Complete the code in `policy_evaluation_ip`. Pseudo-code can be found in the textbook in the box on page 75. Then test your code on `GridWorld-v0` with the uniform policy to see that you still get the correct value function.\n", "\n", "***Note:*** You have already written a function that computes the right-hand side of the Bellman equation!"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def policy_evaluation_ip(env, discount, agent, v0, max_iter=1000, tol=1e-6):\n", "    \n", "    v = v0\n", "    \n", "    for i in range(max_iter): # Loop\n", "        delta = 0\n", "        for s in range(env.observation_space.n):\n", "            vs = v[s]\n", "            \n", "            # Code for updating v[s]\n", "            \n", "            delta = np.max([delta, np.abs(vs-v[s])])\n", "            \n", "        if (delta < tol): # Until delta < tol\n", "            break\n", "            \n", "    return v    "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["env = gym.make('GridWorld-v0')\n", "agent = RandomAgent()\n", "discount = 1\n", "\n", "v0 = np.zeros(16)\n", "v = policy_evaluation_ip(env, discount, agent, v0)\n", "print(v.reshape(4,4))"]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["# 7. Policy Iteration <a id=\"sec7\">"]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["Now when we have code for evaluating a policy, it is time to see how it can be improved. Remember that the idea is to act greedily with respect to $v_{\\pi}(s)$. That is, given $v_{\\pi}(s)$ we can compute $q_{\\pi}(s,a)$, and then the greedy (improved) policy is\n", "$$\n", "\\pi'(s) = \\text{argmax}_{a} q_{\\pi}(s,a)\n", "$$\n", "We have already written code for computing $q_{\\pi}(s,a)$ for a given $v_{\\pi}(s)$, so the only thing we have to do now is to implement the maximization.\n", "\n", "`greedy_policy` will return `a_probs` which encode a policy that is greedy with respect to `v`. That is `a_probs[s][a]` $= \\pi'(a|s)$. Make sure that you understand the code."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def greedy_policy(env, discount, agent, v):\n", "    \n", "    # The new policy will be a_probs\n", "    # We start by setting all probabilities to 0\n", "    # Then when we have found the greedy action in a state, \n", "    # we change the probability for that action to 1.0.\n", "    \n", "    a_probs = np.zeros((env.observation_space.n, env.action_space.n)) \n", "    \n", "    for s in range(env.observation_space.n):\n", "        \n", "        action_values = np.zeros(env.action_space.n)\n", "        \n", "        for a in range(env.action_space.n):\n", "            # Compute action value for all actions\n", "            action_values[a] = compute_action_value(env, discount, s, a, v)\n", "            \n", "        a_max = np.argmax(action_values) # A greedy action\n", "        a_probs[s][a_max] = 1.0 # Always choose the greedy action!\n", "        \n", "    return a_probs"]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["Lets try to improve the policy on `GridWorld-v0`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["env = gym.make('GridWorld-v0')\n", "agent = RandomAgent()\n", "discount = 1\n", "\n", "# We first evaluate the policy\n", "v = np.zeros(env.observation_space.n)\n", "v = policy_evaluation(env, discount, agent, v)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["v_old = v\n", "\n", "# And then we improve the policy (act greedy w.r.t v)\n", "agent.probs = greedy_policy(env, discount, agent, v)\n", "\n", "# We can also evaluate the new policy \n", "v = policy_evaluation(env, discount, agent, v)\n", "\n", "print(\"Value of initial policy:\")\n", "print(v_old.reshape(4,4))\n", "print(\"\\nValue of improved policy:\")\n", "print(v.reshape(4,4))"]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["Assuming that your implementation of `compute_action_value` is correct, \n", "we can clearly see that the improved policy has higher value in every state. In fact, the policy is now an optimal policy. To see this, you can try to rerun the second cell above and note that the policy does not improve anymore.\n", "\n", "**Policy iteration:** However, it is not the case for all environments that the policy will converge in just one improvement. In this case we may have to improve the policy several times until it finally converge to the optimal policy. \n", "\n", "Finally, we can try to run the agent with the improved policy."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["run_agent(env, agent)"]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["**Task:** Find an optimal policy for `FrozenLake-v0`. (Note again that you may have to improve several times to reach an optimal policy!)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["env = gym.make('FrozenLake-v0')\n", "agent = RandomAgent()\n", "discount = 1\n", "# Enter code here\n", "", "", "", "    \n", "", "", "        \n", "", "", "\n", ""]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["# 8. Value iteration <a id=\"sec8\">"]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["In the value iteration we instead start from the Bellman optimality equation\n", "\n", "$$\n", "v_{*}(s) = \\max_{a} q_{\\pi_*}(s,a) = \\max_a \\sum_{s', r} p(s', r | s, a) [r + \\gamma v_{*}(s')]\n", "$$\n", "\n", "We start with an initial guess $v_0$ and then we repeatedly compute the right-hand side of this equation, until we converge to the optimal state-value function. When we have the optimal state-value function $v_*$, we can take any policy that is greedy w.r.t $v_*$ and this will give us an optimal policy. \n", "\n", "**Task 1:** Complete the code below. Pseudo-code for the algorithm can be found on page 83 in the textbook. Note that the code for computing the action-value given $v_{\\pi}$ has already been implemented above.\n", "\n", "The `value_iteration` function will (if implemented correctly) give back the optimal value function. \n", "\n", "**Task 2:** Also add some code for computing the optimal policy given this, and try it on `FrozenLake-v0` and/or `GridWorld-v0`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def value_iteration(env, discount, agent, v0, max_iter=1000, tol=1e-6):\n", "    \n", "    v = v0\n", "    \n", "    for i in range(max_iter): # Loop\n", "        delta = 0\n", "        for s in range(env.observation_space.n):\n", "            vs = v[s]\n", "            \n", "            \n", "            \n", "            # Code for updating v[s]\n", "                \n", "            \n", "            ##\n", "            \n", "            delta = np.max([delta, np.abs(vs-v[s])])\n", "            \n", "        if (delta < tol): # Until delta < tol\n", "            break\n", "            \n", "", "            \n", "    return v    "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["env = gym.make('FrozenLake-v0')\n", "agent = RandomAgent()\n", "discount = 1\n", "\n", "", "", "", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [""]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.7"}}, "nbformat": 4, "nbformat_minor": 4}