{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Tinkering Notebook for Lecture 1 - Introduction\n",
    "\n",
    "In this notebook you will see that your Jupyter environment is working, and also try to use reinforcement learning on a simple problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Table of content\n",
    "* ### [1. Imports](#sec1)\n",
    "* ### [2. A very short introduction to Reinforcement Learning (RL)](#sec2)\n",
    "* ### [3. Getting started with Open AI gym](#sec3)\n",
    " * #### [3.1 Taxi driver](#sec3_1)\n",
    " * #### [3.2 MountainCar environment](#sec3_2)\n",
    "* ### [4. *Multi-armed bandits](#sec4)\n",
    " * #### [4.1 Learn](#sec4_1)\n",
    " * #### [4.2 Act](#sec4_2)\n",
    " * #### [4.3 Testing MultiarmedBandits](#sec4_3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# 1. Imports <a id=\"sec1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "For this notebook you have to import the following packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages needed for this assignment\n",
    "import gym\n",
    "import gym_bandits #Implements 10-armed bandits from Chapter 2 in the textbook\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output # Used to clear the ouput of a Jupyter cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# 2. A very short introduction to Reinforcement Learning (RL) <a id=\"sec2\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "RL is a family of modern machine learning techniques for learning how to make sequential decisions using feedback from real and/or simulated environments. \n",
    "\n",
    "In RL the agent (e.g. computer program) interacts with an environment and gets rewards. The environment could be a physical or chemical system, resource management, traffic light control, advertisement network, a computer game or many other things. The goal of the agent is typically to maximize the cumulative sum of rewards over some number of sequential actions. In order to do so, the agent learns from observations in order to improve its future actions. \n",
    "\n",
    "Some important concepts in RL:\n",
    "* __Agent__: The learner and decision maker.\n",
    "* __Environment__:  What the agent interacts with.\n",
    "* __State__: A state $s \\in \\mathcal{S}$ is a succint representation of the environments current state.\n",
    "* __Action__: The agent can take actions $a \\in \\mathcal{A}$ in order to change the state of the environment.\n",
    "* __Observation__: After each action the agent recieves an observation of the environment. For most of the course we will assume that the agent observs the state $s$.\n",
    "* __Policy__: Rules for how the agent choses the next action given the current state, $a = \\pi(s)$.\n",
    "* __Reward__: An immediate reward $R(s,a)$ that the agent gets for taking action $a$ in state $s$.\n",
    "\n",
    "For more discussion on the meaning of these concepts, see Lecture 1 and the course book. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# 3. Getting started with Open AI gym <a id=\"sec3\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "In supervised learning different methods can be evaluated on static data sets. In RL, however, the algorithms must be tested on interactive (dynamic) environments. This is where OpenAI Gym comes in.\n",
    "\n",
    "[OpenAI Gym](http://gym.openai.com) is a toolkit for comparing RL-algorithms. It contains a wide variety of environments that you can train your agents on, and it is often used for benchmarking new methods in the RL research litterature. \n",
    "There are also [leaderboards](https://github.com/openai/gym/wiki/Leaderboard) for different gym-environments, showing which methods has been most successful so far.\n",
    "\n",
    "In the exercises, \"Tinkering Notebooks\", we will make use of OpenAI Gym.\n",
    "\n",
    "To test your installation of OpenAI gym, and learn about basic usage, we will look at the relatively simple Taxi-environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## 3.1 Taxi driver <a id=\"sec3_1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "In this environment there are four locations. Your job is to pick up a passenger at one of these locations, and then drop her of at another location. \n",
    "\n",
    "To test this environment run the following lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v3')\n",
    "state = env.reset()\n",
    "print('Initial state:', state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The methods used above are:\n",
    "* `make()`: Creates a gym environment object. In this case we use the Taxi-environment.\n",
    "* `reset()`: Resets the environment to an initial state, and returns the initial state. \n",
    "In the case of the Taxi-environment, the initial state is chosen randomly, so it will be different every time you run `env.reset()`.\n",
    "\n",
    "To visualize the current state of the environment, you can use the function `render()`. In the Taxi-environment the visualization is text-based. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The filled square represents the taxi, the letters (R, G, Y and B) represents possible pickup and destination locations, and | represents a wall. The blue letter is the passenger, and the purple is the destination.\n",
    "\n",
    "Next we take a look at the state space $\\mathcal{S}$ (all possible states) and action space $\\mathcal{A}$ (all possible actions). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"State space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "__State space__: Contains all possible states of the environment. We see that the state space here contains 500 discrete states. In this case each state corresponds to a position of the taxi (25 possibilities), the passengers position (5 possibilities, including picked up) and the destination (4 possibilities). Hence, there are $25\\times5 \\times 4 = 500$ possible states.\n",
    "\n",
    "__Action space__: Contains all possible actions the agent can take. In this case the six discrete actions corresponds to: 0 - south, 1 - north, 2 - east, 3 - west, 4 - pickup, 5 - dropoff. \n",
    "\n",
    "***Remark***: Note that we actually asked for the `observation_space` and not the state space. As mentioned above, we will for most of the course assume that the observation space and the state space are the same. However, in some RL-problems the full state cannot be observed, so the space of possible states may not be the same as the space of possible observations. For example, the complete state of an inverted pendulum consists of both the angle and velocity, but maybe only the angle is meausred directly.\n",
    "\n",
    "We next see how the agent can interact with the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state, reward, done, info = env.step(1) # Take action 1 (north)\n",
    "env.render()\n",
    "print(\"New state:\", new_state)\n",
    "print(\"Reward:\", reward)\n",
    "print(\"Done:\", done)\n",
    "print(\"Info:\", info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "If it was possible, the taxi should now have moved one step north (if the taxi started at the top row then it will not move). The step-function returns the following information:\n",
    "* __New state__: The state after the action is taken.\n",
    "* __Reward__: The immediate reward. In the taxi-environment the reward for illegal \"pickup\" or \"dropoff\" is -10, successfully delivering the passenger gives +20, and any other action gives -1.\n",
    "* __Done__: Is the environment done? In the Taxi-environment this will be false until the passenger is successfully dropped at her destination, or the number of actions taken gets larger than 200.\n",
    "* __info__: Additional information mainly used for debugging.\n",
    "\n",
    "The goal of the agent is to maximize the total reward. In this environment this means to deliver the passenger to her destination in as few steps as possible. If more than 200 actions are taken we will stop even if the passenger is not at her destination.\n",
    "\n",
    "One (quite bad) strategy for the taxi problem is to take a random action every time. Inside a gym-environment this can be done using `env.action_space.sample()`, which samples a random action from the action space. Look through the following loop and make sure that you understand whats going on. (We use `clear_output()` to clear the output of the Jupyter cell, and `time.sleep()` to pause between each action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "time_step = 0\n",
    "total_reward = 0\n",
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample() #Choose random action\n",
    "    state, reward, done, info = env.step(action) \n",
    "    total_reward += reward\n",
    "    time_step += 1\n",
    "    clear_output(wait=True)\n",
    "    env.render()\n",
    "    print(\"Time step:\", time_step)\n",
    "    print(\"Reward:\", reward)\n",
    "    print(\"Total reward:\", total_reward)\n",
    "    time.sleep(.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "As you can see, random actions is, unsurprisingly, not a good policy. However, if the agent has no prior information about the environment or the goal, what else could it do?\n",
    "\n",
    "If we know everything about the environment, we could create an array with 500 entries, where each entry tells us what the optimal action is in the corresponding state. One way of finding this array is to use dynamic programming. This will be discussed in Lecture 3.\n",
    "\n",
    "In Lecture 4 and Lecture 5 we will see how the agent can learn the array without prior knowledge, by just observing the reward received for taking different actions in different states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## 3.2 MountainCar environment <a id=\"sec3_2\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "In the course we will use some environments where `render()` uses more advanced graphics. In order to make sure that this works on your installation, we also test the *MountainCar*-environment.\n",
    "\n",
    "In this case, `render()` opens a new window where the current state of the environment is visualized. \n",
    "If you want to close this window you call `env.close()`. If you have overwritten __env__ for some reason, you will have to restart your Jupyter kernel in order to close the window.\n",
    "\n",
    "***Remark:*** We call render two times. This should not be needed, but sometimes on Windows the first call only opens the window without actually rendering the frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "env.reset()\n",
    "env.render()\n",
    "env.render() \n",
    "print(\"State space:\", env.observation_space, \"Low:\", env.observation_space.low, \"High:\", env.observation_space.high)\n",
    "print(\"Action space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "*  __Action space__: We have three discrete actions, 0 - push left, 1 - no push, 2 - push right\n",
    "* __State space__: `Box(2,)` represents a two dimensional box, so it is a continuous space. Each state is a vector with two elements. In MouintainCar the first element is the position of the car, and can go between -1.2 and +0.6. The second element is the velocity of the car and can go between -0.07 and +0.07.\n",
    "* __Reward__: The reward given by the environment is -1 for each action. \n",
    "* __Done__: The environment is done when the flag is reached (position = 0.5), or 200 actions have been used. The goal is thus to reach the flag in as few steps as possible. \n",
    "\n",
    "Lets first try to solve this environment by the simple policy: Always push the car right (action = 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state):\n",
    "    \n",
    "    action = 2\n",
    "        \n",
    "    return action  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "time_step = 0\n",
    "total_reward = 0\n",
    "done = False\n",
    "while not done:\n",
    "    action = policy(state)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    env.render()\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print(\"State:\", state)\n",
    "    print(\"Action:\", action)\n",
    "    print(\"Total reward:\", total_reward)\n",
    "    \n",
    "    time.sleep(.02)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Unfortunately the car does not have the momentum to overcome gravity, and thus it never reaches the flag. The agent must learn that the car needs to gain momentum by going up the left hill.\n",
    "\n",
    "This environment is harder than the Taxi-enivronment for RL-agents. There are two reasons for this:\n",
    "1. The state-space is continuous (infinitely many states). Hence, it is not possible to build up a table with information about all states. In RL this can be solved using function approximations. This will be discussed in the second half of the course.\n",
    "\n",
    "2. The agent always gets the immediate reward -1 for each action until it reaches the flag. So all actions looks just as bad if we do not reach the flag within 200 steps, and it is very unlikely that the car reaches the flag by only using e.g. random actions. So how can it learn about the goal? We will see two possible solutions in the course: We can try to re-engineer the reward so that the agent is encouraged to go up the left slope first. Or more generally, we can encourage the agent to try to reach states that has not been seen before.\n",
    "\n",
    "Even though the problem is hard for an RL-agent that knows nothing about the environment, it is relatively easy for an engineer that knows the basics of the environment to find a policy that reaches the flag within 200 steps. The strength of RL is that the agent can learn these only by interacting with the environment. However, when we know more about the environment it can still be much more efficient to let an engineer figure out a good policy, and then potentially use RL to improve this policy.\n",
    "\n",
    "***\n",
    "### Task:\n",
    "a) Try to construct a policy that only uses the current velocity (`state[1]`) to determine the action. The goal is to reach the flag within 200 steps. Implement your policy in the function `policy` above and try it. Remember to execute the cell with the function after you have made your changes. *Hint:* It is actually enough to check in which direction the car is currently moving.\n",
    "\n",
    "b) Implement the policy that is on the leaderboard for MountainCar-v0: ($p$ is position,  $v$ is velocity, and the state is $s = \\begin{bmatrix} p & v \\end{bmatrix}^\\top$)\n",
    "$$\n",
    "\\pi(s) = \\begin{cases} \\text{Right} & \\text{if } \\min\\{-0.09( p + 0.25)^2 + 0.03, 0.3(p + 0.9)^4 - 0.008\\} \\leq v \\leq -0.07(p+0.38)^2 + 0.07 \\\\ \\text{Left} & \\text{otherwise} \\end{cases}\n",
    "$$\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# 4. *Multi-armed bandits <a id=\"sec4\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "This part of the assignment is based on Chapter 2 in the textbook. It will give you a test of many of the ideas in RL.\n",
    "\n",
    "To get a more complete discussion you can read through Section 2.1-2.6. Below a short summary of these sections is given. After this you can implement and test some relatively simple RL-algorithm.\n",
    "\n",
    "Assume that you have 10 slot machines (one-armed bandits) in front of you. Each slot machine has an expected reward that is unknown to you. You will get to choose between the slot machines 1000 times, and your goal is to get as large total reward as possible. \n",
    "\n",
    "This problem is called a multi-armed bandit problem. Now we formulate this as an RL-problem:\n",
    "* __Action space__: $\\mathcal{A} = \\{ 0, 1, \\ldots, 9\\}$, so action $a=0$ means that we try slot machine $0$, $a=1$ means we try slot machine $1$ etc.\n",
    "* __State space__: Since the slot machines do not change between each action, there is just one state. This simplifies the problem: The environment does not change when an action is taken. Hence, the agent can concentrate on the immediate reward  it gets for each action, and it does not have to worry about how the action changes the state of the environment. \n",
    "* __Reward__: The return $R$ we get from the slot machine we picked. \n",
    "\n",
    "In order to formalize our goal, we define the value of an action, $q_*(a)$, to be the expected reward when $a$ is chosen, i.e., $q_*(a) = \\mathbb{E}[ R | a]$. Note that $q_*(a)$ can be represented by an array with 10 elements, where each element gives the value for the corresponding action.\n",
    "\n",
    "\n",
    "If we know the value function $q_*(a)$, the optimal policy is to always choose the action\n",
    "\n",
    "$$\n",
    "A = \\underset{a}{\\operatorname{argmax}} q_*(a).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## 4.1 Learn <a id=\"sec4_1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "However, we do not know what $q_*(a)$ is. An option is to try $n-1$ actions in order to get an estimate $Q_n(a)$ of the expected reward for each slot machine. A reasonable estimate of the the expected reward for a specific action $a$ is the average reward  seen so far when we have taken action $a$. That is, when $n-1$ actions have been taken, we can compute an estimate of $q_*(a)$ as\n",
    "\n",
    "$$\n",
    "Q_n(a) = \\frac{ \\text{ sum of rewards when $a$ taken prior to $n$} }{\\text{number of times $a$  taken prior to $n$}} \\quad \\text{(see eq 2.1 in textbook)},\n",
    "$$\n",
    "\n",
    "where we let $Q_n(a) = 0$ if the action $a$ was not taken prior to the $n$th action. In particular, $Q_1(a) = 0$ for all $a$.\n",
    "\n",
    "Instead of recomputing the sums every time a new action is taken, we can update this sums incrementally. Equation (2.3) in the textbook shows how to do this. That is, if the $n$th action taken is $A_n$, and the received reward is $R_n$, then $Q_{n+1}(A_n)$ can be computed from $Q_n(A_n)$ as\n",
    "\n",
    "\n",
    "$$\n",
    "Q_{n+1}(A_n) = Q_{n}(A_n) + \\frac{1}{N(A_n)}(R_n - Q_n(A_n))\n",
    "$$\n",
    "where $N(A_n)$ is the number of times that $A_n$ has been taken so far.\n",
    "\n",
    "If we initialize the estimate to $Q_1(a) = 0$ for all $a \\in \\mathcal{A}$, then both expressions for $Q_n(a)$ are equivalent. For implementation the second expression is quite useful. Here we only need to create an array with 10 elements (one for each possible action) and whenever action $a$ is taken and the reward is observed, we update element $a$ in the array. \n",
    "Pseudo-code for this can be written as\n",
    "\n",
    "*Initialize:*\n",
    "\n",
    "For all $a \\in \\mathcal{A}$, \n",
    "  * $\\quad Q(a) \\leftarrow 0$\n",
    "  * $\\quad N(a) \\leftarrow 0$\n",
    "  \n",
    "*Learn:*\n",
    "\n",
    "When action $A$ is taken with received reward $R$, update the estimates:\n",
    "  * $N(A) \\leftarrow N(A) + 1$\n",
    "  * $Q(A) \\leftarrow Q(A) + \\frac{1}{N(A)} (R - Q(A))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## 4.2 Act <a id=\"sec4_2\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Given an estimate $Q(a)$, how should the agent choose the next action? \n",
    "\n",
    "A straightforward answer is to pick the one that maximizes $Q(a)$. This is in RL called the *greedy* choice, picking the action that according to the current estimates seems best.\n",
    "\n",
    "The problem with the greedy choice is that the estimates may be incorrect. If the current estimate of the optimal action's value is too low then the greedy agent will not take that action, and thus never update the estimated value of it. In other word, the agent is not exploring all possibilities. A simple, but often quite effective, way of adding exploration is that the agent use the greedy option most of the time, but with probability $\\varepsilon$ it takes a random action.\n",
    "This is called an $\\varepsilon$-greedy policy. In this way the agent will always continue to explore different possibilities. The $\\varepsilon$-greedy policy given an estimate $Q(a)$ can be written as\n",
    "\n",
    "$$\n",
    " \\begin{cases} \\underset{a}{\\operatorname{argmax}} Q(a) & \\text{with probability } 1-\\varepsilon \\\\ \\text{random action} & \\text{with probability } \\varepsilon\\end{cases}\n",
    "$$\n",
    "\n",
    "When several actions have an estimate equal to the maximum, the agent can break ties any way it wants (e.g. randomly). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## 4.3 Testing MultiarmedBandits <a id=\"sec4_3\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "In OpenAI gym, the multi-armed bandits are implemented in the `MultiarmedBandits-v0` environment. This environment is not in the standard gym package, so to use it you have to  `import gym_bandits`. \n",
    "\n",
    "Below you can see an example of how to use this environment. A class `Agent` is used to implement an agent that does not learn anything, and always chooses a random action. \n",
    "In the exercise below you will change the code in this class in order to implement the methods described above (and in Chapter 2 of the textbook).\n",
    "\n",
    "*Remark:* The environment is by default a 10-armed bandit. If you want e.g. a 15-armed bandit, you can use `env = gym.make('MultiarmedBandits-v0', nr_arms=15)`. If you want to see what the true $q_*(a)$ is, you can use `env.values` (this will of course change everytime you call `env.reset()`). This can be interesting if you want to compare it to the estimated $Q$ that your agent has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MultiarmedBandits-v0\")\n",
    "print('State space:', env.observation_space)\n",
    "print('Action space:', env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__(self, epsilon=0, nr_arms=10):\n",
    "        self.epsilon = epsilon\n",
    "        self.nr_arms = nr_arms\n",
    "        self.N = np.zeros(nr_arms)\n",
    "        self.Q = np.zeros(nr_arms)\n",
    "        \n",
    "    def learn(self, action, reward):\n",
    "        pass\n",
    "        \n",
    "    \n",
    "    def act(self):\n",
    "        action = np.random.choice(self.nr_arms) # Random action\n",
    "            \n",
    "        \n",
    "            \n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "agent = Agent(epsilon = 0.1)\n",
    "rewards = np.empty(1000)\n",
    "for t in range(1000):\n",
    "    action = agent.act()\n",
    "    state, reward, done, info = env.step(action)\n",
    "    rewards[t] = reward\n",
    "    agent.learn(action, reward)\n",
    "\n",
    "print('Total reward: %.2f' % np.sum(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the reward given for each action\n",
    "plt.plot(rewards)\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Reward');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "***\n",
    "### Exercise\n",
    "\n",
    "a) Change the `Agent`-class in order to implement an $\\varepsilon$-greedy agent. To do this, change the `learn`-method to implement the update of $Q$ given an action with corresponding reward. Then implement the $\\varepsilon$-greedy policy in the `act`-function.  \n",
    "\n",
    "b) Try to run your agent with e.g. $\\varepsilon = 0.1$.\n",
    "\n",
    "d) When you have tested that your implementation, you can run the code below the reproduce plots like in Figure 2.2 in the textbook. Here we do 2000 runs using the agent, and then take the average reward at different time steps. The code may take some minute to run!\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.1 # Change this to try different values\n",
    "rewards = np.zeros((2000, 1000)) \n",
    "\n",
    "for i in range(2000):\n",
    "    agent = Agent(epsilon = epsilon)\n",
    "    env.reset()\n",
    "    \n",
    "    for t in range(1000):\n",
    "        action = agent.act()\n",
    "        state, reward, done, info = env.step(action)\n",
    "        rewards[i,t] = reward\n",
    "        agent.learn(action, reward) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rewards = np.mean(rewards, 0)\n",
    "plt.plot(mean_rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
